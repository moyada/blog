---
title: 机器学习入门
categories: 机器学习
date: 2017-04-07 22:04:18
tags:
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<big>机器学习是，在特定的编程环境下，给予计算机学习能力的领域。</big>

# 监督学习(Supervised Learning)
监督学习的目的是根据已有数据给出结果，并能预测出离散值的输出
监督学习所针对的问题分为<big>__回归问题(Regression)__</big>和<big>__分类问题(Classification)__</big>
简单来说回归问题主要为线性回归，是通过一组连续的数据，得到某个函数，并以这个函数来预测某个输入所映射的输出。例如价格预估。
而分类问题是通过一组离散的数据，来预测出某个输入的输出结果。

# 无监督学习(Unsupervised Learning)
无监督学习的主要方式是聚类算法，将一组有关联的数据划分为一类，与分类问题不同的是，聚类算法只是单纯的分组并不得到数据的输出结果
，就像搜索引擎那样只是将关键字的基本信息返回。

# 代价函数(Cost Function)
代价函数也称之假设函数(hypothesis function)
它是根据数据中的平均差异来确定出的最接近真实值的函数。
$$J(θ0,θ1) = \frac{1}{2m}\sum_{i=1}^m(hθ(x_i)−y_i)^2$$
其中J(θ0,θ1)为预测值表达式，(θ0,θ1)为最小化区域，m为数据数量，\\(x_i\\)代表实际输入，\\(y_i\\)代表实际输出，hθ(x)为预测值，hθ(x) = θ0+θ1x。
通过预测输出hθ(xi)与实际输出yi的平方差异J(θ0,θ1)的最小值求出代价函数。

# 梯度下降(Gradient Descent)
梯度下降的思想是对于函数J(θ0,θ1)，从相同的θ0，θ1为起点，逐步改变θ0，θ1，直到找到J(θ0,θ1)的最小值。
梯度下降的轮廓图是由一组凸函数(convex function)组成，凸函数类似个弓形函数，全部组合类似一副山脉的全览图，而它的过程就类似一个下山的最短路径选择。
$$θj:=θj−α(\frac{∂}{∂θj})J(θ0,θ1)$$
α为学习速率，\\((\frac{∂}{∂θj})J(θ0,θ1)\\)为导数项
学习速度为梯度下降的步长，导数项将得出切线作为梯度下降的方向。
当学习速率非常小时，梯度下降所花费的时间将增长；当学习速度非常大时，梯度下降的切线斜率与正确的斜率将存在较大的差异。

对于线性回归，导数项可结合代价函数:
\\(α(\frac{∂}{∂θj})J(θ0,θ1)\\)

\\(= α(\frac{∂}{∂θj})\*1/2(hθ(x)−y)^2\\)

\\(= α(\frac{∂}{∂θj})\*1/2(θ0+θ1x−y)^2\\)

\\(= 2 \* 1/2(hθ(x)−y)\*\frac{∂}{∂θj}(hθ(x)−y)\\)

\\(= hθ(x)−y \* (\frac{∂}{∂θj})(\sum_{i=1}^m(θ_ix_i−y))\\)

\\(= (hθ(x)−y)\*(xi)\\)

微积分hθ(xi)就是J(θ0,θ1)的斜率

\\(α(\frac{∂}{∂θj})J(θ0,θ1)\\) \\(= 1/m\sum_{i=0}^m(hθ(x_i)−y_i)\\)

\\(α(\frac{∂}{∂θj})J(θ0,θ1)\\) = \\(1/m\sum_{i=1}^m(hθ(x_i)−y_i)*(x_i)\\)

\\(θ0:=θ0-α1/m\sum_{i=1}^m\\) \\((hθ(x_i)−y_i)\\)

\\(θ1:=θ1-α1/m\sum_{i=1}^m\\) \\(((hθ(x_i)−y_i)*(x_i))\\)

# 多特征参数(Multiple Feature)
当输入参数不止一个时，代价函数变形为
\\(hθ(x) = θ_0+θ_1x_1+θ_2x_2+...+θ_nx_n\\)

对于多参数的代价函数，可以对其进行简化。
定义一个\\(x_0 = 1\\)，得:
\\(hθ(x) = θ_0x_0+θ_1x_1+θ_2x_2+...+θ_nx_n\\)
定义两个矢量:
X = \\(\begin{bmatrix}
x0 & x1 & x2 & ... & xn \\
\end{bmatrix}\\)
θ = \\(\begin{bmatrix}
θ0 & θ1 & θ2 & ... θn\\
\end{bmatrix}\\)
多组多特征值的训练样本的预估值可用表达式: \\(hθ(x) = θ^TX\\) 来表示
同理梯度下降函数:

$$θn:=θn-α\frac{1}{m}\sum_{i=1}^m((hθ(x^{(i)})−y^{(i)})*x_n^{(i)})$$

# 特征缩放(Feature Scaling)
将特征值约束至某一范围内(-1< x <1)
均值归一化(normalization)
$$x_i \Leftarrow \frac{(x_i - u_i)}{s_i}$$
其中\\(u_i\\)为平均值，\\(s_i\\)为特征值范围，也就是最大值减去最小值

eg: 
	X = \\(\begin{bmatrix} 1 & 2 & 3 & ... & 10 \\ \end{bmatrix}\\)
	\\(u_i = \frac{(1+2+3+...+10)}{10} = 5.5\\)
	\\(s_i = 10-1 = 9\\)

# 学习速率(Learning Rate)
学习速率α可以通过J(θ)与迭代步数的曲线来推测。
如果α过小，则曲线收敛太慢，下降太快；
如果α过大，则曲线收敛趋于平稳或增长。

# 多项式回归(Polynomial Regression)
对于多个特征值，有时候并不需要使用全部特征值来获得预测值。
这时候，可以对代价函数进行变形为多项式方程:
\\(hθ(x) = θ0+θ1(feture)+θ2(feture^2)+θ3(feture^3)\\)
或 \\(hθ(x) = θ0+θ1(feture)+θ2(feture^{1/2})\\)

# 正规方程(Normal Equation)
相对于剃度下降，正规方程可以一次求出范围内的θ
假设拥有m组多特征值训练样本X，对应m个实际值
对特征值X的矩阵(m,n)添加了\\(x_0=1\\))使维度变化为(m,n+1)
可得出使函数最小化的θ:

\\(θ = (X^TX)^{-1}\\)\\(X^T y\\)

优点：不需要学习速率α，不需要迭代
缺点：需要计算\\((X^TX)^{-1}\\)，特征值维度大的话将计算的非常慢

如果\\(X^TX\\)是不可逆的情况，那么可对部分特征值进行舍去，从而使裁减后的特征值紧密的关联。

对于特征值维度上万的复杂情况，建议使用梯度下降来收敛函数

# 逻辑回归(Logistic Regression)
在分类问题上常见的解决方式是逻辑回归。
在二元分类的问题上，假设实际值y属于0或1，那么hθ(x)的特征值区间范围在(-∞,0) || (1,∞)
而逻辑回归则是对于 0 <= x <= 1区间内，预测输出y=1的概率hθ(x)
\\(hθ(x) = g(θ^Tx)	\\)
\\(g(z) = 1/(1+e^{-z})\\)
=>
\\(hθ(x) = 1/(1+e^{(-θ^Tx)})\\)

# 决策边界(Decision Boundary)
假设对于分类问题上有一条线明显区分不同的预估值，这条线就是决策边界。
根据输出概率得出:
\\(hθ(x)≥0.5 \to y=1\\)
\\(hθ(x)<0.5 \to y=0\\)

根据逻辑回归函数可得:
\\(z=0,e^0=1 \Rightarrow g(z)= 1/2\\)
\\(z \to ∞,e^{−∞} \to 0 \Rightarrow g(z)=1\\)
\\(z \to −∞,e^∞ \to ∞ \Rightarrow g(z)=0\\)

则决策边界为:
\\(hθ(x) = g(θ^Tx) = 1/(1+e^{(-θ^Tx)}) = 0.5\\)
\\(z = θ^Tx = 0\\)

\\(θ^Tx≥0 \Rightarrow y=1\\)
\\(θ^Tx<0 \Rightarrow y=0\\)

# 





